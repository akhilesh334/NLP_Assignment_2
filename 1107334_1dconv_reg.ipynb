{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1107334_1dconv_reg.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPi3FiAw8DXj5oRdFfrJR07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilesh334/NLP_Assignment_2/blob/master/1107334_1dconv_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv4cswdC3cXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "28003305-ad5f-44af-ff7e-8918eeaad188"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir(\"gdrive/My Drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRsvRTnt3tRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "6b00c1b6-b6df-4d86-a525-60edf788cbde"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "url_tr=\"https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv\"\n",
        "data=pd.read_csv(url_tr,sep='\\t')\n",
        "data.head(20)\n",
        "#data.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>of escapades demonstrating the adage that what...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>of</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades demonstrating the adage that what is...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrating the adage that what is good for ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrating the adage</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrating</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>the adage</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>the</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>adage</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>that</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>what</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    PhraseId  ...  Sentiment\n",
              "0          1  ...          1\n",
              "1          2  ...          2\n",
              "2          3  ...          2\n",
              "3          4  ...          2\n",
              "4          5  ...          2\n",
              "5          6  ...          2\n",
              "6          7  ...          2\n",
              "7          8  ...          2\n",
              "8          9  ...          2\n",
              "9         10  ...          2\n",
              "10        11  ...          2\n",
              "11        12  ...          2\n",
              "12        13  ...          2\n",
              "13        14  ...          2\n",
              "14        15  ...          2\n",
              "15        16  ...          2\n",
              "16        17  ...          2\n",
              "17        18  ...          2\n",
              "18        19  ...          2\n",
              "19        20  ...          2\n",
              "\n",
              "[20 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rPN8Ic93ueu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "340255ea-17c7-4cb8-a6d6-cd285d624989"
      },
      "source": [
        "numSentences = data['SentenceId'].max()\n",
        "fullSentences = []\n",
        "curSentence = 0\n",
        "for i in range(data.shape[0]):\n",
        "  if data['SentenceId'][i]> curSentence:\n",
        "    fullSentences.append((data['Phrase'][i], data['Sentiment'][i]))\n",
        "    curSentence = curSentence +1\n",
        "\n",
        "len(fullSentences)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6hn2W3B31H8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "046deab6-0c82-49e8-a89e-b7b850a880b7"
      },
      "source": [
        "fullSentDf = pd.DataFrame(fullSentences,\n",
        "                                columns=['Phrase', 'Sentiment'])\n",
        "data['Sentiment'].value_counts()\n",
        "fullSentDf['Sentiment'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2325\n",
              "1    2203\n",
              "2    1659\n",
              "4    1282\n",
              "0    1075\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tt0QFP034jD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "2723b254-8132-4c48-a88a-6b17e144e390"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLWjpTpw39kn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3feb868f-221a-42ce-9aa2-491e637258be"
      },
      "source": [
        "documents = []\n",
        "for i in range(fullSentDf.shape[0]):\n",
        "  tmpWords = word_tokenize(fullSentDf['Phrase'][i])\n",
        "  documents.append((tmpWords, fullSentDf['Sentiment'][i]))\n",
        "\n",
        "random.seed(9001)\n",
        "random.shuffle(documents)\n",
        "print(documents[1][0])\n",
        "len(documents)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['For', 'its', '100', 'minutes', 'running', 'time', ',', 'you', \"'ll\", 'wait', 'in', 'vain', 'for', 'a', 'movie', 'to', 'happen', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrFMIhrD4Dq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5dc212c8-dd76-45d7-c2b3-df6fdb3110f7"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations=\"?:!.,;'\\\"-()\"\n",
        "\n",
        "#parameters to adjust to see the impact on outcome\n",
        "remove_stopwords = True\n",
        "useStemming = True\n",
        "useLemma = False\n",
        "removePuncs = True\n",
        "\n",
        "for l in range(len(documents)):\n",
        "  label = documents[l][1]\n",
        "  tmpReview = []\n",
        "  for w in documents[l][0]:\n",
        "    newWord = w\n",
        "    if remove_stopwords and (w in stopwords_en):\n",
        "      continue\n",
        "    if removePuncs and (w in punctuations):\n",
        "      continue\n",
        "    if useStemming:\n",
        "      #newWord = porter.stem(newWord)\n",
        "      newWord = lancaster.stem(newWord)\n",
        "    if useLemma:\n",
        "      newWord = wordnet_lemmatizer.lemmatize(newWord)\n",
        "    tmpReview.append(newWord)\n",
        "  documents[l] = (' '.join(tmpReview), label)\n",
        "print(documents[2])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('ian holm conqu frant earthy napoleon', 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BfqXqgY4P7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "074135fb-e783-4f68-c534-e8be6a6a61e5"
      },
      "source": [
        "all_data = pd.DataFrame(documents,\n",
        "                                columns=['text', 'sentiment'])\n",
        "x_train_raw, x_test_raw, y_train_raw, y_test_raw = train_test_split(all_data['text'], all_data['sentiment'], test_size=0.3)\n",
        "len(x_train_raw)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5980"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui7EpEYp4Wlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f78cadf-baae-4bb7-ea11-4aae05fa8926"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "x_train = vectorizer.fit_transform(x_train_raw)\n",
        "y_train = y_train_raw\n",
        "x_test = vectorizer.transform(x_test_raw)\n",
        "y_test = y_test_raw\n",
        "\n",
        "x_train_np = x_train.toarray()\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_test_np = x_test.toarray()\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "x_train_np.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5980, 50397)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5rP6gEL4kSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(2020)\n",
        "from torch.nn import Conv1d\n",
        "from torch.nn import MaxPool1d\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import Linear\n",
        "from torch.nn.functional import relu, softmax, sigmoid\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uIv40HR4odc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CnnClassifier(torch.nn.Module):\n",
        "  def __init__(self, batch_size, inputs, outputs):\n",
        "    super(CnnClassifier, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.inputs = inputs\n",
        "    self.outputs = outputs\n",
        "    self.input_layer = Conv1d(inputs, batch_size, 1)\n",
        "    self.max_pooling_layer = MaxPool1d(1)\n",
        "    self.conv_layer = Conv1d(batch_size, 128, 1)\n",
        "    self.flatten_layer = Flatten()\n",
        "    self.linear_layer = Linear(128, 64)\n",
        "    self.output_layer = Linear(64, outputs)\n",
        "  def feed(self, input):\n",
        "    input = input.reshape((self.batch_size, self.inputs, 1))\n",
        "    output = relu(self.input_layer(input))\n",
        "    output = self.max_pooling_layer(output)\n",
        "    output = relu(self.conv_layer(output))\n",
        "    output = self.flatten_layer(output)\n",
        "    output = self.linear_layer(output)\n",
        "    output = self.output_layer(output)\n",
        "    output = softmax(output)\n",
        "   \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-TGadSC4unR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import SGD, Adam\n",
        "from torch.nn import L1Loss, CrossEntropyLoss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzAKcAch45wx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "3621553e-160e-4846-c0aa-3ee8ac03cce5"
      },
      "source": [
        "batch_size = 128\n",
        "model = CnnClassifier(batch_size, x_train.shape[1], 5)\n",
        "model.cuda()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CnnClassifier(\n",
              "  (input_layer): Conv1d(50397, 128, kernel_size=(1,), stride=(1,))\n",
              "  (max_pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_layer): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "  (flatten_layer): Flatten()\n",
              "  (linear_layer): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (output_layer): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wieFYaOj5Cdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return (correct.sum() / torch.FloatTensor([y.shape[0]])) / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf-QDdXn_DUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_precision(preds, y):\n",
        "  max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "  correct = max_preds.squeeze(1).eq(y)\n",
        "  return (correct.sum() / torch.FloatTensor(y.shape[0])) / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc0vAbiG6IE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f37afc52-da6c-457e-bde0-035c2f10828d"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "indicies=range(5)\n",
        "encoder=LabelBinarizer()\n",
        "labels=encoder.fit_transform(indicies)\n",
        "print(labels[1])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49xWg9CO6K_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_loss(model, dataset, train = False, optimizer = None):\n",
        "  performance = L1Loss()\n",
        "  criterion = CrossEntropyLoss()\n",
        "  avg_loss = 0\n",
        "  avg_accu = 0\n",
        "  avg_prec = 0\n",
        "  count = 0\n",
        "  \n",
        "  for input, output in iter(dataset):\n",
        "    predictions = model.feed(input)\n",
        "    loss = performance(predictions, output)\n",
        "    tmp_prec = categorical_precision(predictions, output)\n",
        "    tmp_accu = categorical_accuracy(predictions, output)\n",
        "    \n",
        "    if(train):\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    avg_loss += loss.item()\n",
        "    avg_accu += tmp_accu.item()\n",
        "    avg_prec += tmp accu.item()\n",
        "    count += 1\n",
        "    \n",
        "  return avg_loss / count, avg_accu / count, avg_prec / count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bf3v6J96aJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "838c01cc-5654-4aa2-ce69-ead5c9e3423f"
      },
      "source": [
        "epochs = 25\n",
        "optimizer = SGD(model.parameters(), lr=1e-5)\n",
        "inputs = torch.from_numpy(x_train_np).cuda().float()\n",
        "outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()\n",
        "tensor = TensorDataset(inputs, outputs)\n",
        "loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n",
        "for epoch in range(epochs):\n",
        "  avg_loss, avg_accu = model_loss(model, loader, train=True, optimizer=optimizer)\n",
        "  print(\"Epoch \" + str(epoch + 1) + \":\\n\\tLoss = \" + str(avg_loss))\n",
        "  print(\"Accuracy = \" + str(avg_accu))\n",
        "  print(\"Precision= \"+ str(avg_prec))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-dacd965248cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_accu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\\n\\tLoss = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_accu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKyCnSQ07HHG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0a0e12d2-76f3-456a-c699-7111839d07d1"
      },
      "source": [
        "#saving model to disk\n",
        "torch.save(model, '/content/gdrive/My Drive/assignment/1107334_1Dconv_reg.ipynb')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CnnClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnWuJ_M87WoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "38d5da02-9018-471d-f440-de59deaa43a8"
      },
      "source": [
        "model = torch.load('/content/gdrive/My Drive/assignment/1107334_1Dconv_reg.ipynb')\n",
        "model.eval()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CnnClassifier(\n",
              "  (input_layer): Conv1d(50397, 128, kernel_size=(1,), stride=(1,))\n",
              "  (max_pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_layer): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "  (flatten_layer): Flatten()\n",
              "  (linear_layer): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (output_layer): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyl8nruA7cRE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5ae087da-1938-4f9a-fbf4-7616287222dc"
      },
      "source": [
        "inputs = torch.from_numpy(x_test_np).cuda().float()\n",
        "outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()\n",
        "tensor = TensorDataset(inputs, outputs)\n",
        "loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n",
        "avg_loss, avg_accu = model_loss(model, loader)\n",
        "print(\"The model's L1 loss is: \" + str(avg_loss))\n",
        "print(\"The model's Accuracy is: \" + str(avg_accu))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The model's L1 loss is: 1.9290625512599946\n",
            "The model's Accuracy is: 0.1234375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}